---
title: "Profanity Detection"
description: "Learn how to use JigsawStack's Profanity Detection API to identify and filter inappropriate content"
---

# Profanity Detection

## Overview

The Profanity Detection API helps you identify and filter inappropriate language, slurs, and offensive content in text. Designed for content moderation across various platforms, our API goes beyond simple keyword matching to understand context and detect obfuscated profanity.

**Key Benefits:**
- High-accuracy detection of explicit and implicit profanity
- Context-aware analysis that understands intent
- Detection of obfuscated profanity and character substitutions
- Multiple language support
- Customizable sensitivity levels
- Optional content masking

## Use Cases

### Content Moderation
Filter user-generated content on social platforms, forums, and comments sections.

### Child Safety
Create safe environments for child-friendly applications and educational platforms.

### Brand Protection
Ensure marketing content and customer interactions maintain brand-appropriate language.

## API Endpoint

```
POST /v1/validate/profanity
```

## Quick Start

<RequestExample>
```javascript Javascript
import { JigsawStack } from "jigsawstack";

const jigsaw = JigsawStack({ apiKey: "your-api-key" });

const response = await jigsaw.profanity({
  text: "This is a clean message."
});

console.log(response);
```

```python Python
from jigsawstack import JigsawStack

jigsaw = JigsawStack(api_key="your-api-key")

response = jigsaw.validate.profanity({
  "text": "This is a clean message."
})

print(response)
```

```bash cURL
curl -X POST https://api.jigsawstack.com/v1/validate/profanity \
  -H "Content-Type: application/json" \
  -H "x-api-key: your-api-key" \
  -d '{
    "text": "This is a clean message."
  }'
```
</RequestExample>

## Response Example

```json
{
  "success": true,
  "message": "Validated successfully",
  "clean_text": "This product is absolutely terrible. What the **** were they thinking?",
  "profanities": ["f***"],
  "profanities_found": 1
}
```

## Request Parameters

### Body Parameters

**text** (string, required)
- The text content to analyze for profanity.
- **Constraints:** Maximum 10,000 characters

**censor_replacement** (string, default: "*")
- The character to replace profane words with.
- Example: If set to "*", "bad word" would become "*** ****"

### Headers

**x-api-key** (string, required)
- Your JigsawStack API key

## Response Structure

**success** (boolean)
- Indicates whether the API call was successful.

**message** (string)
- A message indicating the result of the operation.

**clean_text** (string)
- The original text with profanity replaced by the censor_replacement character.

**profanities** (array)
- An array of profane words found in the text.

**profanities_found** (integer)
- The number of profanities found in the text.

## Best Practices

### Content Moderation Pipeline

Implement a complete content moderation workflow:

```javascript
// Example: Complete content moderation pipeline
async function moderateContent(userContent) {
  // Step 1: Check for profanity
  const profanityResult = await jigsaw.validate.profanity({
    text: userContent,
    censor_replacement: "*"
  });
  
  // Step 2: If content has profanity, use the censored version
  let processedContent = profanityResult.profanities_found > 0 
    ? profanityResult.clean_text 
    : userContent;
  
  // Step 3: Add flags for content that may need manual review
  const needsReview = profanityResult.profanities_found > 3;
  
  // Step 4: Analyze sentiment on clean content
  const sentimentResult = await jigsaw.ai.sentiment({
    text: processedContent
  });
  
  return {
    original: userContent,
    processed: processedContent,
    contains_profanity: profanityResult.profanities_found > 0,
    profanity_count: profanityResult.profanities_found,
    sentiment: sentimentResult.sentiment,
    needs_review: needsReview
  };
}
```

### Handling User-Generated Content at Scale

For platforms with high volume of user content:

```javascript
// Example: Batch processing user-generated content
async function processUserSubmissions(submissions) {
  // Process in batches to avoid rate limiting
  const batchSize = 10;
  const results = [];
  
  for (let i = 0; i < submissions.length; i += batchSize) {
    const batch = submissions.slice(i, i + batchSize);
    const promises = batch.map(async submission => {
      try {
        const profanityCheck = await jigsaw.validate.profanity({
          text: submission.content,
          censor_replacement: "*"
        });
        
        return {
          id: submission.id,
          result: profanityCheck,
          action: determineAction(profanityCheck)
        };
      } catch (error) {
        console.error(`Error processing submission ${submission.id}:`, error);
        return {
          id: submission.id,
          error,
          action: "manual_review"
        };
      }
    });
    
    const batchResults = await Promise.all(promises);
    results.push(...batchResults);
    
    // Avoid rate limiting with a small delay between batches
    if (i + batchSize < submissions.length) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
  
  return results;
}

function determineAction(profanityCheck) {
  if (profanityCheck.profanities_found > 5) {
    return "reject";
  } else if (profanityCheck.profanities_found > 2) {
    return "flag_for_review";
  } else if (profanityCheck.profanities_found > 0) {
    return "use_censored_version";
  }
  return "approve";
}
```

## Example Applications

### Chat Application Filter

```javascript
// Example: Real-time chat message filter
function ChatApp() {
  const [message, setMessage] = useState("");
  const [isSending, setIsSending] = useState(false);
  
  const handleSendMessage = async () => {
    if (!message.trim()) return;
    
    setIsSending(true);
    
    try {
      // Check message for profanity before sending
      const profanityCheck = await jigsaw.validate.profanity({
        text: message,
        censor_replacement: "*"
      });
      
      if (profanityCheck.profanities_found > 3) {
        // High profanity count - don't send and alert user
        alert("Your message contains inappropriate language and cannot be sent.");
        return;
      } else if (profanityCheck.profanities_found > 0) {
        // Contains some profanity - confirm with user to send censored version
        const willSendCensored = confirm(
          "Your message contains some inappropriate language. " +
          "Would you like to send a filtered version instead?"
        );
        
        if (willSendCensored) {
          await sendMessageToChat(profanityCheck.clean_text);
        } else {
          return;
        }
      } else {
        // No profanity - send as is
        await sendMessageToChat(message);
      }
      
      // Clear input after sending
      setMessage("");
    } catch (error) {
      console.error("Error sending message:", error);
      alert("Failed to send message. Please try again.");
    } finally {
      setIsSending(false);
    }
  };
  
  return (
    // Chat UI components...
  );
}
```

### Content Moderation Dashboard

```javascript
// Example: Admin dashboard for content moderation
async function ContentModerationDashboard() {
  // Get content that needs moderation
  const pendingContent = await fetchPendingContent();
  
  // Process each content item
  const processedItems = await Promise.all(
    pendingContent.map(async item => {
      // Run profanity check
      const profanityCheck = await jigsaw.validate.profanity({
        text: item.content,
        censor_replacement: "*"
      });
      
      // Run sentiment analysis
      const sentimentCheck = await jigsaw.ai.sentiment({
        text: item.content
      });
      
      // Determine recommended action
      let recommendedAction = "approve";
      
      if (profanityCheck.profanities_found > 5) {
        recommendedAction = "reject";
      } else if (profanityCheck.profanities_found > 2) {
        recommendedAction = "review";
      } else if (sentimentCheck.sentiment.label === "negative" && sentimentCheck.sentiment.score > 0.8) {
        recommendedAction = "review";
      }
      
      return {
        ...item,
        profanityCheck,
        sentimentCheck,
        recommendedAction
      };
    })
  );
  
  // Group content by recommended action
  const groupedContent = {
    approve: processedItems.filter(item => item.recommendedAction === "approve"),
    reject: processedItems.filter(item => item.recommendedAction === "reject"),
    review: processedItems.filter(item => item.recommendedAction === "review")
  };
  
  // Render dashboard with content groups
  return groupedContent;
}
```

### Implementing Multi-Layer Content Filtering

```javascript
// Example: Multi-layer content filtering system
async function multiLayerContentFilter(content) {
  // Layer 1: Basic profanity filter
  const profanityResult = await jigsaw.validate.profanity({
    text: content,
    censor_replacement: "*"
  });
  
  // If no profanity detected, check for subtle inappropriate content
  if (profanityResult.profanities_found === 0) {
    // Layer 2: Check for more subtle inappropriate content using AI
    try {
      // This could be a custom AI-based content moderation API
      const aiModeration = await jigsaw.ai.sentiment({
        text: content
      });
      
      // Example detection of potentially harmful content
      const harmfulEmotions = ['anger', 'hatred', 'disgust'];
      const hasHarmfulEmotion = harmfulEmotions.some(
        emotion => aiModeration.sentiment.emotions && 
                  aiModeration.sentiment.emotions[emotion] > 0.7
      );
      
      if (hasHarmfulEmotion) {
        return {
          status: "flagged",
          reason: "potentially_harmful_sentiment",
          content: content,
          needs_review: true
        };
      }
    } catch (error) {
      console.error("AI moderation failed:", error);
      // Fall back to basic filtering if AI check fails
    }
  }
  
  // Apply appropriate action based on profanity detection
  if (profanityResult.profanities_found > 5) {
    return {
      status: "rejected",
      reason: "excessive_profanity",
      content: null,
      needs_review: false
    };
  } else if (profanityResult.profanities_found > 0) {
    return {
      status: "modified",
      reason: "profanity_detected",
      content: profanityResult.clean_text,
      needs_review: profanityResult.profanities_found > 3
    };
  }
  
  // Content passed all checks
  return {
    status: "approved",
    reason: "passed_all_filters",
    content: content,
    needs_review: false
  };
}
```

## Frequently Asked Questions

### How effective is the profanity detection for non-English content?

The profanity detection API is most effective for English content, but also provides good coverage for major languages including Spanish, French, German, Italian, and Portuguese. For other languages, basic profanity detection is available but may not catch language-specific slang or culturally specific offensive terms.

### Can the API detect deliberately obfuscated profanity?

Yes, the API is designed to detect common obfuscation techniques such as:
- Character substitution (e.g., "sh!t", "f**k")
- Letter omission (e.g., "sh-t", "fck")
- Deliberate misspellings (e.g., "phuck")
- Zero-width space insertion

However, very creative or unusual obfuscation methods might occasionally bypass detection.

### How can I reduce false positives?

If you're experiencing false positives (benign words incorrectly flagged as profanity), consider these approaches:
1. Post-process the results to whitelist certain terms specific to your domain
2. Implement a user feedback system to identify false positives
3. Add a human review step for edge cases
