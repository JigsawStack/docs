---
title: "Speech to Text"
api: "POST https://api.jigsawstack.com/v1/ai/transcribe"
description: "Transcribe video and audio files with ease leveraging Whisper large V3 AI model."
---

import SpeechToTextSnippet from "/snippets/code-req-examples/speech-to-text.mdx";

## Supported Formats and Limitations

- **Supported formats:** MP3, WAV, M4A, FLAC, AAC, OGG, WEBM
- **Maximum file size:** 100MB
- **Maximum audio duration:** 4 hours

## Request Parameters

### Body

<ParamField body="url" type="string">
  The video/audio url. Not required if `file_store_key` is specified.
</ParamField>

<ParamField body="file_store_key" type="string">
  The key used to store the video/audio file on Jigsawstack File [Storage](/docs/api-reference/store/file/add). Not required if `url` is specified.
</ParamField>

<Info>Either `url` or `file_store_key` should be provided, not both.</Info>

<ParamField body="language" type="string">
  The language to transcribe or translate the file into. If not specified, the model will automatically detect the language and transcribe
  accordingly. All supported language codes can be found [here](https://jigsawstack.com/docs/additional-resources/languages).
</ParamField>

<ParamField body="translate" type="boolean" default="false">
  When set to true, translates the content into English (or the specified language if `language` parameter is provided). All supported language codes
  can be found [here](https://jigsawstack.com/docs/additional-resources/languages).
</ParamField>

<ParamField body="by_speaker" type="boolean" default="false">
  Identifies and separates different speakers in the audio file. When enabled, the response will include a `speakers` array with speaker-segmented
  transcripts.
</ParamField>

<ParamField body="webhook_url" type="string">
  Webhook URL to send result to. When provided, the API will process asynchronously and send results to this URL when completed.
</ParamField>

<ParamField body="batch_size" type="number" default="30">
  The batch size to return. Maximum value is 40. This controls how the audio is chunked for processing.
</ParamField>

<Snippet file="header.mdx" />

## Response Structure

### Direct Response

<ResponseField name="success" type="boolean">
  Indicates whether the call was successful.
</ResponseField>

<ResponseField name="text" type="string">
  The complete transcribed text from the audio/video file.
</ResponseField>

<ResponseField name="chunks" type="array">
  An array of transcript chunks with timestamps.
  <Expandable title="Chunk Object">
    <ResponseField name="timestamp" type="array[number]">
      Array containing start and end time in seconds for the chunk.
    </ResponseField>
    <ResponseField name="text" type="string">
      The transcribed text for this time segment.
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="speakers" type="array">
  Only present when `by_speaker` is set to true. Contains speaker-segmented transcripts.
  <Expandable title="Speaker Object">
    <ResponseField name="speaker" type="string">
      The speaker identifier (e.g., "Speaker 1").
    </ResponseField>
    <ResponseField name="timestamp" type="array[number]">
      Array containing start and end time in seconds for this segment.
    </ResponseField>
    <ResponseField name="text" type="string">
      The transcribed text spoken by this speaker.
    </ResponseField>
  </Expandable>
</ResponseField>

### Webhook Response

When using `webhook_url`, the initial response will be different:

<ResponseField name="success" type="boolean">
  Indicates whether the request was successfully queued.
</ResponseField>

<ResponseField name="status" type="string">
  Will be "processing" when the transcription job is queued successfully.
</ResponseField>

<ResponseField name="id" type="string">
  A unique identifier for the transcription job.
</ResponseField>

The complete transcription result will later be sent to your webhook URL with the same structure as the direct response.

## Advanced Features

### Speaker Diarization

Speaker diarization is the process of separating an audio stream into segments according to the identity of each speaker. When you enable the `by_speaker` parameter, the API will:

1. Transcribe the audio as usual
2. Identify distinct speakers in the recording
3. Label each segment with a speaker identifier (e.g., "SPEAKER_1", "SPEAKER_2")
4. Return both the standard chunks and a separate `speakers` array with speaker-separated transcriptions

This is particularly useful for:

- Meeting transcriptions
- Interview transcriptions
- Podcast transcriptions
- Any multi-speaker audio content

### Webhook Usage

For long audio files, processing might take some time. Instead of keeping the connection open and waiting for the result, you can provide a `webhook_url` parameter. The API will:

1. Return immediately with a job ID
2. Process the audio asynchronously
3. Send the complete transcription results to your webhook URL when finished

Make sure your webhook endpoint is set up to:

- Accept POST requests
- Parse JSON content
- Handle the same response format as the standard API response

<SpeechToTextSnippet />
